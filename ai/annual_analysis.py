import os
import json
from tools.data_fetcher import set_dataset
from tools.genChart import generate_time_series_chart
from collections import Counter
from tools.anomaly_detection import anomaly_detection
import datetime
import logging
from swarm import Swarm, Agent
from urllib.parse import quote

# ------------------------------
# Initialization
# ------------------------------
# Initialize OpenAI API key
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OpenAI API key not found in environment variables.")

client = Swarm()

# GPT_MODEL = 'gpt-3.5-turbo-16k'
GPT_MODEL = 'gpt-4o'

# ------------------------------
# Helper Functions
# ------------------------------

def load_combined_data(datasets_folder, log_file):
    combined_data = []
    article_list = [f for f in os.listdir(datasets_folder) if f.endswith('.json')]

    if not article_list:
        print(f"No JSON files found in {datasets_folder}")
        return combined_data

    print(f"Found {len(article_list)} JSON files to process.")

    for filename in article_list:
        article_path = os.path.join(datasets_folder, filename)
        print(f"\nProcessing file: {filename}")
        try:
            with open(article_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, dict):
                    combined_data.append(data)
                elif isinstance(data, list) and data:
                    combined_data.extend(data)
                else:
                    print(f"No valid data structure found in {filename}")
        except json.JSONDecodeError as e:
            print(f"JSON decode error in file {filename}: {e}")
            continue
        except Exception as e:
            print(f"Error reading file {filename}: {e}")
            continue

    if combined_data:
        print(f"Total records before sorting: {len(combined_data)}")
        combined_data.sort(key=lambda x: (str(x.get('report_category', 'Unknown')),
                                         str(x.get('periodic', 'Unknown'))))
    else:
        print("No valid data found in the files.")

    return combined_data

def save_combined_html(combined_data, output_folder):
    output_file = os.path.join(output_folder, 'all_data.html')
    with open(output_file, 'w', encoding='utf-8') as html_file:
        html_file.write("<table border='1'><tr><th>Index</th><th>Category</th><th>Title</th><th>Usefulness</th></tr>")
        for index, entry in enumerate(combined_data):
            category = entry.get('report_category', 'Unknown')
            title = entry.get('title', 'Unknown')
            periodic = entry.get('periodic', 'Unknown')
            district_level = entry.get('district_level', 'Unknown')
            html_file.write(f"<tr><td>{index}</td><td>{category}</td><td>{title}</td><td>{periodic}</td><td>{district_level}</td></tr>")
        html_file.write("</table>")
    print(f"Combined data saved to {output_file}")

def process_entry(index, data_entry, output_folder, log_file, script_dir):
    title = data_entry.get('title', 'Unknown')
    noun = data_entry.get('item_noun', data_entry.get('table_metadata', {}).get('item_noun', 'Unknown'))
    category = data_entry.get('report_category', 'Unknown')
    endpoint = data_entry.get('endpoint', None)
    query = data_entry.get('queries', {}).get('Yearly', None)
    date_fields = data_entry.get('DateFields', [])
    numeric_fields = data_entry.get('NumericFields', [])
    category_field = data_entry.get('CategoryFields', [])
    location_field = data_entry.get('LocationFields', [])   
    usefulness = data_entry.get('usefulness', data_entry.get('table_metadata', {}).get('usefulness', 0))
    description = data_entry.get('description', data_entry.get('table_metadata', {}).get('description', ''))
    
    # Check usefulness
    if usefulness == 0:
        log_file.write(f"{index}: {title}({endpoint}) - Skipped due to zero usefulness.\n")
        return

    # Skip if mandatory fields are missing
    if not query or not endpoint or not date_fields or not numeric_fields:
        log_file.write(f"{index}: {title}({endpoint}) - Missing required fields. Skipping.\n")
        return

    # Replace 'month' with 'year' in date_fields if present
    date_fields = ['year' if field == 'month' else field for field in date_fields]

    # Extract table and column metadata
    column_metadata = data_entry.get('columns', [])

    # Set up anomaly detection parameters
    recent_period = {
        'start': datetime.date(2024, 1, 1),
        'end': datetime.date(2024, 12, 31)
    }
    comparison_period = {
        'start': datetime.date(2014, 1, 1),
        'end': datetime.date(2023, 12, 31)
    }

    # Initialize filter conditions (excluding supervisor_district for now)
    filter_conditions = [
        {'field': date_fields[0], 'operator': '<=', 'value': recent_period['end']},
        {'field': date_fields[0], 'operator': '>=', 'value': comparison_period['start']},
    ]

    # Determine if supervisor_district is present
    has_supervisor_district = 'supervisor_district' in location_field

    try:
        context_variables = {}  # Initialize context_variables for each iteration
        # Set the dataset
        # Replace start_date with Jan 1 of ten years ago:
        ten_years_ago = datetime.date.today().year - 11
        query_modified = query.replace(' start_date', f"'{datetime.date(ten_years_ago, 1, 1)}'")
        result = set_dataset(context_variables=context_variables, endpoint=endpoint, query=query_modified, filter_conditions=filter_conditions)
        if 'error' in result:
            log_file.write(f"{index}: {title} ({endpoint}) - Error setting dataset: {result['error']}\n")
            return
        # Get the dataset from context_variables
        if 'dataset' in context_variables:
            dataset = context_variables['dataset']
        else:
            log_file.write(f"{index}: {title} ({endpoint}) - No dataset found in context.\n")
            return
        
        if 'queryURL' in result:
            log_file.write(f"{index}: {title} ({endpoint}) - Query URL: {result['queryURL']}\n")
            query_url = result['queryURL']
            
        all_markdown_contents = []
        all_html_contents = []
        print(f"Date fields from JSON: {date_fields}")
        print(f"Numeric fields from JSON: {numeric_fields}")
        print(f"Category field from JSON: {category_field}")
        print(f"Location field from JSON: {location_field}")
        print(f"Available columns in dataset: {context_variables['dataset'].columns.tolist()}")

        # Function to generate charts and anomalies
        def generate_reports(current_dataset, current_filter_conditions, current_output_folder, current_title_suffix, metadata):
            nonlocal all_markdown_contents, all_html_contents
            # Generate charts for each combination of date and numeric fields

            for date_field in date_fields:
                for numeric_field in numeric_fields:
                    # Update chart title for this combination
                    context_variables['chart_title'] = (
                        f"{title} <br> {'count' if numeric_field == 'item_count' else numeric_field.replace('_', ' ')} by {date_field}"
                    )
                    context_variables['noun'] = f"{title}"
                    # Generate the chart
                    chart_result = generate_time_series_chart(
                        context_variables=context_variables,
                        time_series_field=date_field,
                        numeric_fields=numeric_field,
                        aggregation_period='year',
                        max_legend_items=10,
                        filter_conditions=current_filter_conditions,
                        show_average_line=True
                    )
                    # Ensure we're adding strings, not tuples or dicts
                    if isinstance(chart_result, tuple):
                        markdown_content, html_content = chart_result
                        all_markdown_contents.append(str(markdown_content))
                        all_html_contents.append(str(html_content))
                    elif isinstance(chart_result, dict):
                        all_html_contents.append(str(chart_result.get('html', '')))
                    else:
                        all_html_contents.append(str(chart_result))

            # Loop through each category field to detect anomalies
            for cat_field in category_field:
                try:
                    context_variables['chart_title'] = (
                        f"{title} <br> {'count' if numeric_fields[0] == 'item_count' else numeric_fields[0].replace('_', ' ')} by {date_fields[0]} by {cat_field}"
                    )
                    chart_result = generate_time_series_chart(
                        context_variables=context_variables,
                        time_series_field=date_fields[0],
                        numeric_fields=numeric_fields[0],
                        aggregation_period='year',
                        max_legend_items=10,
                        group_field=cat_field,
                        filter_conditions=current_filter_conditions,
                        show_average_line=False
                    )
                    if isinstance(chart_result, tuple):
                        markdown_content, html_content = chart_result
                        all_markdown_contents.append(str(markdown_content))
                        all_html_contents.append(str(html_content))
                    elif isinstance(chart_result, dict):
                        all_html_contents.append(str(chart_result.get('html', '')))
                    else:
                        all_html_contents.append(str(chart_result))

                    print(f"Detecting anomalies for category field: {cat_field}")

                    # Check anomalies for each numeric field
                    for numeric_field in numeric_fields:
                        print(f"Detecting anomalies for numeric field: {numeric_field}")

                        anomalies_result = anomaly_detection(
                            context_variables=context_variables,
                            group_field=cat_field,
                            filter_conditions=current_filter_conditions,
                            min_diff=2,
                            recent_period=recent_period,
                            comparison_period=comparison_period,
                            date_field=date_fields[0],
                            numeric_field=numeric_field,
                            y_axis_label=numeric_field,
                            title=context_variables['chart_title']
                        )

                        if anomalies_result and 'anomalies' in anomalies_result:
                            anomalies_html = anomalies_result['anomalies']
                            all_html_contents.append(anomalies_html)
                            anomalies_markdown = anomalies_result.get('anomalies_markdown', '')
                            all_markdown_contents.append(anomalies_markdown)

                except Exception as e:
                    print(f"Error detecting anomalies for category {cat_field} in entry at index {index}: {str(e)}")
                    import traceback
                    print(traceback.format_exc())
                    # Log the error and move on
                    log_file.write(f"{index}: {title} - Error detecting anomalies: {str(e)}\n")
                    continue

            # After generating all charts and anomalies, save the files
            # Prepare the HTML content
            processed_html_contents = []
            for content in all_html_contents:
                if content is None:
                    continue
                if isinstance(content, dict):
                    if 'html' in content:
                        processed_html_contents.append(str(content['html']))
                    else:
                        processed_html_contents.append(str(content))
                else:
                    processed_html_contents.append(str(content))

            combined_html = "\n\n".join(processed_html_contents)

            # Prepare metadata for HTML
            metadata_html = f"<head><title>{metadata['noun']} {current_title_suffix}</title></head><body><h1>10 year look-back for {metadata['noun']} {current_title_suffix}</h1>\n"
            metadata_html += f"<p><strong>Query URL:</strong> <a href='{query_url}'>LINK</a></p>\n"
            metadata_html += f"<p><strong>{metadata['description']}</strong></p>"
            metadata_html += f"<p><strong>{metadata['endpoint']}</strong></p>"
            metadata_html += f"<p><strong>{metadata['title']}</strong></p>"
            metadata_html += "<h2>Column Metadata</h2><table border='1'><tr><th>Field Name</th><th>Description</th><th>Data Type</th></tr>"
            for column in metadata['column_metadata']:
                row = "<tr>"
                if isinstance(column, dict):
                    # Assume fieldName, description, dataTypeName keys exist
                    row += f"<td>{column.get('fieldName', 'Unknown')}</td>"
                    row += f"<td>{column.get('description', 'No description')}</td>"
                    row += f"<td>{column.get('dataTypeName', 'Unknown')}</td>"
                else:
                    # If column is a string
                    row += f"<td>{column}</td><td>No description</td><td>Unknown</td>"
                row += "</tr>"
                metadata_html += row
            metadata_html += "</table>"

            full_html_content = metadata_html + "\n\n" + combined_html

            # Sanitize title for filename
            sanitized_title = endpoint if endpoint.endswith('.json') else endpoint + '.json'
            if current_title_suffix != "City Wide":
                sanitized_title = f"{sanitized_title.replace('.json', '')}_{current_title_suffix.lower().replace(' ', '_')}.json"

            # Save HTML file to the appropriate folder
            html_filename = os.path.join(current_output_folder, f"{sanitized_title}.html")
            os.makedirs(os.path.dirname(html_filename), exist_ok=True)
            if os.path.exists(html_filename):
                os.remove(html_filename)
            with open(html_filename, 'w', encoding='utf-8') as f:
                f.write(full_html_content)

            # Process and save markdown content similarly
            processed_markdown_contents = []
            for content in all_markdown_contents:
                if content is None:
                    continue
                if isinstance(content, dict):
                    if 'markdown' in content:
                        processed_markdown_contents.append(str(content['markdown']))
                    else:
                        processed_markdown_contents.append(str(content))
                else:
                    processed_markdown_contents.append(str(content))

            combined_markdown = "\n\n".join(processed_markdown_contents)
            metadata_md = f"# {metadata['noun']} {current_title_suffix}\n\n"
            metadata_md += f"**Query URL:** {query_url}\n\n"
            metadata_md += f"**Description:** {metadata['description']}\n\n"
            metadata_md += "## Column Metadata\n\n| Field Name | Description | Data Type |\n|------------|-------------|-----------|\n"
            for column in metadata['column_metadata']:
                if isinstance(column, dict):
                    field_name = column.get('fieldName', 'Unknown')
                    description = column.get('description', 'No description')
                    data_type = column.get('dataTypeName', 'Unknown')
                    metadata_md += f"| {field_name} | {description} | {data_type} |\n"
                else:
                    metadata_md += f"| {column} | No description | Unknown |\n"

            full_markdown_content = metadata_md + "\n\n" + combined_markdown
            markdown_filename = os.path.join(current_output_folder, f"{sanitized_title}.md")
            if os.path.exists(markdown_filename):
                os.remove(markdown_filename)
            with open(markdown_filename, 'w', encoding='utf-8') as f:
                f.write(full_markdown_content)

            # Clear the contents for the next iteration
            all_markdown_contents.clear()
            all_html_contents.clear()

            # Return the html filename for logging
            return html_filename

        # Create metadata dictionary
        metadata = {
            'title': title,
            'noun': noun,
            'description': description,
            'endpoint': endpoint,
            'column_metadata': column_metadata
        }

        # Process unfiltered data
        main_html_file = generate_reports(dataset, filter_conditions, output_folder, "City Wide", metadata)

        # If supervisor_district is present, process for each district
        if has_supervisor_district:
            for district in range(1, 12):  # Districts 1-11
                district_output = os.path.join(output_folder, 'districts', f'district_{district}')
                
                os.makedirs(district_output, exist_ok=True)
                district_filter_conditions = filter_conditions + [
                    {'field': 'supervisor_district', 'operator': '=', 'value': district}
                ]
                generate_reports(dataset, district_filter_conditions, district_output, f"District {district}", metadata)

        # Log success using the main HTML file path
        relative_html_path = os.path.relpath(main_html_file, start=script_dir)
        log_file.write(f"{index}: {title} ({endpoint}) - Success. Output HTML: {relative_html_path}\n")

    except Exception as e:
        import traceback
        traceback_str = traceback.format_exc()
        print(f"Error processing data from entry at index {index}: {e}")
        print(traceback_str)
        log_file.write(f"{index}: {title}({endpoint}) - Error: {str(e)}\n")

def process_entries(combined_data, num_start, num_end, output_folder, log_file_path, script_dir):
    with open(log_file_path, 'a', encoding='utf-8') as log_file:
        for index in range(num_start, num_end):
            if index >= len(combined_data):
                break
            data_entry = combined_data[index]
            # Process each entry once; district handling is managed within process_entry
            process_entry(index, data_entry, output_folder, log_file, script_dir)

def export_for_endpoint(endpoint, output_folder=None, log_file_path=os.path.join('logs', 'annual_process.log')):
    """
    Export processing for a specific endpoint.

    Args:
        endpoint (str): The endpoint to process.
        output_folder (str, optional): The base output folder. Defaults to None.
        log_file_path (str, optional): Path to the log file. Defaults to None.
    """
    # Paths
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_folder = os.path.join(script_dir, 'data')
    datasets_folder = os.path.join(data_folder, 'datasets/fixed')
    if not output_folder:
        output_folder = os.path.join(script_dir, 'output')

    # Ensure the output directory exists
    os.makedirs(output_folder, exist_ok=True)

    # Create a log file to track the processing results
    if not log_file_path:
        log_file_path = os.path.join(output_folder, "processing_log.txt")
    with open(log_file_path, "a", encoding="utf-8") as log_file:
        log_file.write(f"\nProcessing Export for Endpoint: {endpoint} at {datetime.datetime.now()}\n")
    
    print(f"Processing Export for Endpoint: {endpoint} at {datetime.datetime.now()}")
    # Load combined data
    combined_data = load_combined_data(datasets_folder, log_file_path)

    # Find records matching the endpoint
    matching_records = [entry for entry in combined_data if entry.get('endpoint') == endpoint or entry.get('endpoint') == endpoint + '.json']

    if not matching_records:
        print(f"No records found for endpoint: {endpoint}")
        with open(log_file_path, "a", encoding="utf-8") as log_file:
            log_file.write(f"No records found for endpoint: {endpoint}\n")
        return

    print(f"Found {len(matching_records)} record(s) for endpoint: {endpoint}")

    # Process each matching record
    with open(log_file_path, "a", encoding="utf-8") as log_file:
        for record in matching_records:
            index = combined_data.index(record)
            # Process each entry once; district handling is managed within process_entry
            process_entry(index, record, output_folder, log_file, script_dir)

    print(f"\nExport processing complete for endpoint: {endpoint}.")
    print(f"Log file location: {log_file_path}")

# ------------------------------
# Main Function
# ------------------------------

def main():
    # Paths
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_folder = os.path.join(script_dir, 'data')
    datasets_folder = os.path.join(data_folder, 'datasets/fixed')
    output_folder = os.path.join(script_dir, 'output', 'annual')

    # Ensure the output directory exists
    os.makedirs(output_folder, exist_ok=True)

    # Set the range of entries to process
    num_start = 120
    num_end = 122

    # Create a log file to track the processing results
    log_file_path = os.path.join(output_folder, "processing_log.txt")
    with open(log_file_path, "w", encoding="utf-8") as log_file:
        log_file.write("Processing Log:\n")

    # Load combined data
    combined_data = load_combined_data(datasets_folder, log_file_path)

    # Save combined data as HTML
    if combined_data:
        save_combined_html(combined_data, output_folder)
    else:
        print("No valid data found to save.")

    # Process entries from num_start to num_end
    process_entries(combined_data, num_start, num_end, output_folder, log_file_path, script_dir)

    print(f"\nProcessing complete for entries from index {num_start} to {num_end - 1}.")
    print(f"Log file location: {log_file_path}")

# ------------------------------
# Entry Point
# ------------------------------
if __name__ == '__main__':
    main()
